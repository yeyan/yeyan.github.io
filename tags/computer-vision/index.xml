<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Computer Vision on Ye&#39;s ML Cafe</title>
    <link>https://yeyan.github.io/tags/computer-vision/</link>
    <description>Recent content in Computer Vision on Ye&#39;s ML Cafe</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 09 Dec 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://yeyan.github.io/tags/computer-vision/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Deep Learning with PyTorch (In Progress)</title>
      <link>https://yeyan.github.io/posts/deep-learning/</link>
      <pubDate>Wed, 09 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://yeyan.github.io/posts/deep-learning/</guid>
      <description>Neural network (NN) is one of the hottest topics today. Scientists have invented lots of specialized variations of them: Long/Short Term Memory (LSTM), Gated Recurrent Unit (GRU), Deep Convolutional Network (DCN), Auto Encoder (AE) and etc. All of those are essentially different ways of constructing parametric functional approximations. In this post, I would like to introduce some basics about networks.
1. Neural Network to the Bare-Bones The simplest neural network actually has its own name: Logistic Regression.</description>
    </item>
    
  </channel>
</rss>