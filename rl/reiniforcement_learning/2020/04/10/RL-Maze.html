<!DOCTYPE html>
<html lang="en"><head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Beat Mazes with Reinforcement Learning | Ye’s ML Cafe</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Beat Mazes with Reinforcement Learning" />
<meta name="author" content="Ye Yan" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="If I have to select a type of machine learning that impresses me the most, reinforcement learning (RL) is definitely the answer. In fact, RL probably is the one that have the most potential to become general AI." />
<meta property="og:description" content="If I have to select a type of machine learning that impresses me the most, reinforcement learning (RL) is definitely the answer. In fact, RL probably is the one that have the most potential to become general AI." />
<link rel="canonical" href="/rl/reiniforcement_learning/2020/04/10/RL-Maze.html" />
<meta property="og:url" content="/rl/reiniforcement_learning/2020/04/10/RL-Maze.html" />
<meta property="og:site_name" content="Ye’s ML Cafe" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-04-10T00:00:00+10:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Ye Yan"},"description":"If I have to select a type of machine learning that impresses me the most, reinforcement learning (RL) is definitely the answer. In fact, RL probably is the one that have the most potential to become general AI.","headline":"Beat Mazes with Reinforcement Learning","dateModified":"2020-04-10T00:00:00+10:00","datePublished":"2020-04-10T00:00:00+10:00","url":"/rl/reiniforcement_learning/2020/04/10/RL-Maze.html","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"/rl/reiniforcement_learning/2020/04/10/RL-Maze.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/styles.css">
    <script type="text/javascript" async
            src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
    </script><link type="application/atom+xml" rel="alternate" href="/feed.xml" title="Ye's ML Cafe" /></head>
<body><nav class="navbar is-black">
    <div class="container">
        <div class="navbar-brand">
            <a class="navbar-item" href="/">
                Ye&#39;s ML Cafe
            </a>
            <div class="navbar-burger burger" data-target="navMenuColorblack-example">
                <span></span>
                <span></span>
                <span></span>
            </div>
        </div>

        <div id="navMenuColorblack-example" class="navbar-menu">
            <div class="navbar-start"><a class="navbar-item" href="/about/">
                    About
                </a></div>

        </div>
    </div>
</nav>
<div class="level">
    <img class="image" src="/assets/banner.jpg"></img>
</div>
<main class="container">
            <div class="columns ">
                <div class="column is-four-fifth">
                    <h1 class="title level">Beat Mazes with Reinforcement Learning</h1>
<h1 class="subtitle level">Apr 10, 2020</h1>
<article class="content">
    <p>If I have to select a type of machine learning that impresses me the most, reinforcement learning (RL) is definitely the answer. In fact, RL probably is the one that have the most potential to become general AI.</p>

<p>But the main idea behind RL is actually very simple. In this blog post let’s build a RL agent that can beat randomly generated mazes to demonstrate this idea.</p>

<h4 id="generate-random-maze">Generate Random Maze</h4>

<p>To start, we need a maze. And honestly manually draw maze is pretty painful, therefore we are going to generate one randomly. There are many algorithm that can generate mazes: Depth-first search, Recursive backtracker, Randomized Kruskal’s algorithm and etc.</p>

<p>I have picked a visually pleasing one: Recursive division method. It divides an empty zone into four zones, and then add 3 passages to the walls we just added, which makes sure all the zones are connected. Then divide all the sub-zones recursively until the zone is too small.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">generate_maze</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">random</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="p">):</span>
    <span class="c1"># make map: 1 means passage and 0 means wall
</span>    <span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>

    <span class="c1"># create enclosing wall
</span>    <span class="n">data</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">:]</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">data</span><span class="p">[:,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">rnd</span><span class="p">(</span><span class="o">*</span><span class="n">arange</span><span class="p">,</span> <span class="n">grp</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="n">space</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">*</span><span class="n">arange</span><span class="p">)</span>
        <span class="n">space</span> <span class="o">=</span> <span class="n">space</span><span class="p">[</span><span class="n">space</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="n">grp</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">space</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">proc</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
        <span class="n">w</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span>
        <span class="k">assert</span> <span class="n">w</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">h</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="s">'Maze size can only be odd number!'</span>

        <span class="k">if</span> <span class="n">w</span> <span class="o">&gt;=</span> <span class="mi">3</span> <span class="ow">and</span> <span class="n">h</span> <span class="o">&gt;=</span> <span class="mi">3</span><span class="p">:</span>
            <span class="c1"># divide into 4 zones
</span>            <span class="n">x</span> <span class="o">=</span> <span class="n">rnd</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">grp</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">rnd</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">grp</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

            <span class="n">data</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">data</span><span class="p">[:,</span> <span class="n">y</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="n">passages</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
                <span class="p">(</span><span class="n">rnd</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">y</span><span class="p">),</span>
                <span class="p">(</span><span class="n">rnd</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">w</span><span class="p">),</span> <span class="n">y</span><span class="p">),</span>
                <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">rnd</span><span class="p">(</span><span class="n">y</span><span class="p">)),</span>
                <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">rnd</span><span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">h</span><span class="p">)),</span>
            <span class="p">])</span>

            <span class="n">data</span><span class="p">[</span><span class="nb">tuple</span><span class="p">(</span><span class="n">passages</span><span class="p">[</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="bp">False</span><span class="p">)]</span><span class="o">.</span><span class="n">T</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">1</span>

            <span class="c1"># divide subzones
</span>            <span class="n">proc</span><span class="p">(</span><span class="n">data</span><span class="p">[:</span><span class="n">x</span><span class="p">,</span> <span class="p">:</span><span class="n">y</span><span class="p">])</span>
            <span class="n">proc</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:,</span> <span class="p">:</span><span class="n">y</span><span class="p">])</span>
            <span class="n">proc</span><span class="p">(</span><span class="n">data</span><span class="p">[:</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:])</span>
            <span class="n">proc</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:,</span> <span class="n">y</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:])</span>

    <span class="c1"># set entrance and exit
</span>    <span class="n">data</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="c1"># add internal walls
</span>    <span class="n">proc</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">data</span></code></pre></figure>

<p>Here is an maze output for dimension 101, 101.</p>

<p><img src="/assets/2020-04-10/make_maze.gif" alt="image" /></p>

<h3 id="make-a-simulation-environment">Make a Simulation Environment</h3>

<p>In RL, we educate our agent by stick and carrot. If an agent have done something right we give it a positive reward, if not we punish it by a negative reward. The goal of an agent is to maximize the sum of discounted rewards.</p>

<p>In our maze world, every step the agent is receiving a small negative reward, which will encourage the agent the find the shortest way to the exit. When agent arrive the exit, it will not able to receive any future punishment after that. Therefore the optimization algorithm will try to find a fastest way to end its suffer.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">SimpleMazeWorld</span><span class="p">:</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">random</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="n">shape</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">generate_maze</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">random</span><span class="o">=</span><span class="n">random</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">*</span> <span class="o">-</span><span class="mf">.1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># We can have many way of ending, for example: a trap that agent falls in and can never get out.
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">terminals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([(</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span><span class="mi">2</span><span class="p">)])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

    <span class="o">@</span><span class="nb">property</span>
    <span class="k">def</span> <span class="nf">state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="s">"convience function for calculate the state"</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">ravel_multi_index</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pos</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="s">"reset agent after episode ends"</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pos</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
        <span class="s">"interactive step"</span>
        <span class="k">assert</span> <span class="n">action</span> <span class="o">&gt;=</span><span class="mi">0</span> <span class="ow">and</span> <span class="n">action</span> <span class="o">&lt;=</span> <span class="mi">3</span><span class="p">,</span> <span class="s">'Where are you going?'</span>

        <span class="c1"># convience functions
</span>        <span class="k">def</span> <span class="nf">is_done</span><span class="p">():</span>
            <span class="k">return</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pos</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">terminals</span><span class="p">)</span><span class="o">.</span><span class="nb">all</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="nb">any</span><span class="p">()</span>
        <span class="k">def</span> <span class="nf">reward</span><span class="p">():</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">pos</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">is_done</span><span class="p">():</span>
            <span class="c1"># agent can not receive any reward after it arrive the exit
</span>            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">(),</span> <span class="n">is_done</span><span class="p">(),</span> <span class="p">{}</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos</span>
            <span class="n">w</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span>

            <span class="n">dx</span><span class="p">,</span> <span class="n">dy</span> <span class="o">=</span> <span class="p">[</span>
                <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">+</span><span class="mi">1</span><span class="p">),</span>
            <span class="p">][</span><span class="n">action</span><span class="p">]</span>

            <span class="c1"># make sure our agent does not move off grid or into walls
</span>            <span class="n">xp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">dx</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">w</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">yp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="n">dy</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">xp</span><span class="p">,</span> <span class="n">yp</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">xp</span>
                <span class="n">y</span> <span class="o">=</span> <span class="n">yp</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">pos</span> <span class="o">=</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">(),</span> <span class="n">is_done</span><span class="p">(),</span> <span class="p">{}</span></code></pre></figure>

<h3 id="make-a-agent-with-q-learning">Make a Agent with Q Learning</h3>

<p>As stated before, the goal of an agent is to maximize sum of discounted rewards.
The first question we need to answer is how to measure a given policy <script type="math/tex">\pi</script>, as if we can’t measure it, we can’t manage it.</p>

<p>The most straight forward way is to use expected discounted sum of rewards to measure the quality of policy <script type="math/tex">\pi</script>.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
v_\pi(s)
&= E_\pi[R_{t+1} + \gamma R_{t+2} + \dots + \gamma ^N R_N] \\
&= E_\pi[R_{t+1} + \gamma v_\pi(S_{t + 1}) \vert S_t = s, A_t = \pi(s)] \\
where &
\\
& R_t \text{ is the reward received at time } t
\end{aligned} %]]></script>

<p>However knowing <script type="math/tex">v_\pi</script> alone is not enough, as an agent only know with policy <script type="math/tex">\pi</script> which state it want to be in the most. But it does not know which action to take to be in that state. Therefore we need another function:</p>

<script type="math/tex; mode=display">q_\pi(s, a) = E[R_t + \gamma v_\pi(S_{t+1}) \vert S_t = s, A_t = a]</script>

<p>That is an agent take an arbitrary action at time <script type="math/tex">t</script>, and then use <script type="math/tex">\pi</script> to make subsequence decisions. Then to obtain the optimal policy <script type="math/tex">\pi_*</script>, all we need is to have a function:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{aligned}
q_*(s, a) &= \underset{\pi}{\max}q_\pi(s, a)
\\&= E[R_{t + 1} + \gamma \underset{\pi}{\max} v_\pi(S_{t + 1}) \vert S_t = s, A_t = a]
\\&= \underset{s', r}{\sum}p(s', r| s, a)[r + \gamma \underset{\pi}{\max}v_\pi(s')]
\\&= \underset{s', r}{\sum}p(s', r| s, a)[r + \gamma \underset{a'}{\max}q_*(s', a')]
\end{aligned} %]]></script>

<p>Which is also called Bellman optimality equation for <script type="math/tex">q_*</script>. And yes, that is also where the name Q in Q Learning comes from. Now, we should have enough math to implement the Q learning agent.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">random</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">12345</span><span class="p">)</span>
<span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">)</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">SimpleMazeWorld</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">random</span><span class="p">)</span>

<span class="c1"># we are using linear epsilon decay strategy
</span><span class="n">espilon_space</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>

<span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">product</span><span class="p">(</span><span class="n">shape</span><span class="p">),</span> <span class="mi">4</span><span class="p">))</span>

<span class="c1"># we don't actually need that many iterations, feel free to reduce it.
</span><span class="n">nepoch</span> <span class="o">=</span> <span class="mi">2000</span>
<span class="c1"># we don't want a single episode takes too long
</span><span class="n">max_steps</span> <span class="o">=</span> <span class="mi">300</span>

<span class="c1"># discount factor
</span><span class="n">gamma</span> <span class="o">=</span> <span class="mi">1</span>
<span class="c1"># learning rate
</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.05</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">nepoch</span><span class="p">)):</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon_space</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">epsilon_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">max_steps</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">space</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">4</span><span class="p">)[</span><span class="n">Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="p">:]</span> <span class="o">==</span> <span class="n">Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="nb">max</span><span class="p">()]</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">space</span><span class="p">)</span>

        <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">is_done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

        <span class="c1"># Bellman optimality equation
</span>        <span class="n">expected</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">Q</span><span class="p">[</span><span class="n">next_state</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="nb">max</span><span class="p">()</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">is_done</span><span class="p">)</span>

        <span class="c1"># this is something  actually simmiliar to gradent descend
</span>        <span class="n">Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">expected</span> <span class="o">-</span> <span class="n">Q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">])</span>

        <span class="k">if</span> <span class="n">is_done</span><span class="p">:</span>
            <span class="k">break</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span></code></pre></figure>

<p>After the model trained, here is how our agent behaves:</p>

<p><img src="/assets/2020-04-10/q_learning.gif" alt="image" /></p>

<h3 id="learn-more">Learn More</h3>

<p>In this post, I have brief explained the idea behind RL. If you are interested and would like to read more about it, please checkout <a href="http://incompleteideas.net/book/the-book-2nd.html">Sutton and Barto’s Book</a>.</p>

</article>

                </div>
                <div class="column is-one-fifth"><div class="container">
    <div class="author has-text-centered">
        <div class="media-content">
            <p class="title is-4">Ye Yan</p>
            <p class="subtitle is-6">Sr. Data Scientist</p>
        </div>
        <div class="social level">
            <div class="level-left">
                
                <div class="level-item">
                    <a class="image is-16x16" href="https://github.com/yeyan">
                        <svg viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414"><path d="M8 0C3.58 0 0 3.582 0 8c0 3.535 2.292 6.533 5.47 7.59.4.075.547-.172.547-.385 0-.19-.007-.693-.01-1.36-2.226.483-2.695-1.073-2.695-1.073-.364-.924-.89-1.17-.89-1.17-.725-.496.056-.486.056-.486.803.056 1.225.824 1.225.824.714 1.223 1.873.87 2.33.665.072-.517.278-.87.507-1.07-1.777-.2-3.644-.888-3.644-3.953 0-.873.31-1.587.823-2.147-.09-.202-.36-1.015.07-2.117 0 0 .67-.215 2.2.82.64-.178 1.32-.266 2-.27.68.004 1.36.092 2 .27 1.52-1.035 2.19-.82 2.19-.82.43 1.102.16 1.915.08 2.117.51.56.82 1.274.82 2.147 0 3.073-1.87 3.75-3.65 3.947.28.24.54.73.54 1.48 0 1.07-.01 1.93-.01 2.19 0 .21.14.46.55.38C13.71 14.53 16 11.53 16 8c0-4.418-3.582-8-8-8"/></svg>

                    </a>
                </div>
                
                
                <div class="level-item">
                    <a class="image is-16x16" href="https://www.linkedin.com/in/ye-yan-83921a154">
                        <svg viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg" fill-rule="evenodd" clip-rule="evenodd" stroke-linejoin="round" stroke-miterlimit="1.414"><path d="M13.632 13.635h-2.37V9.922c0-.886-.018-2.025-1.234-2.025-1.235 0-1.424.964-1.424 1.96v3.778h-2.37V6H8.51v1.04h.03c.318-.6 1.092-1.233 2.247-1.233 2.4 0 2.845 1.58 2.845 3.637v4.188zM3.558 4.955c-.762 0-1.376-.617-1.376-1.377 0-.758.614-1.375 1.376-1.375.76 0 1.376.617 1.376 1.375 0 .76-.617 1.377-1.376 1.377zm1.188 8.68H2.37V6h2.376v7.635zM14.816 0H1.18C.528 0 0 .516 0 1.153v13.694C0 15.484.528 16 1.18 16h13.635c.652 0 1.185-.516 1.185-1.153V1.153C16 .516 15.467 0 14.815 0z" fill-rule="nonzero"/></svg>

                    </a>
                </div>
                
            </div>
        </div>
    </div>

    <aside class="menu">
        <p class="menu-label">
            Archive
        </p>

        <ul class="menu-list">
            
            
            <li>
                <a>Apr 2020</a>
                <ul>
                    
                    <li><a href="/rl/reiniforcement_learning/2020/04/10/RL-Maze.html">Beat Mazes with Reinforcement Learning</a></li>
                    
                </ul>
            </li>
            
        </ul>
    </aside>
</div>
</div>
            </div>
        </main><footer class="footer">
    <div class="content has-text-centered">
        <p>
            <strong>Ye's ML Cafe</strong> by <a>{"name"=>"Ye Yan", "job_title"=>"Sr. Data Scientist"}</a>.
            The source code is licensed
            <a href="http://opensource.org/licenses/mit-license.php">MIT</a>. The website content
            is licensed <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY NC SA 4.0</a>.
        </p>
    </div>
</footer>
</body>

</html>
